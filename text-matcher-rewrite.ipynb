{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from nltk.corpus import names\n",
    "from nltk.metrics.distance import edit_distance as editDistance\n",
    "import nltk\n",
    "import re \n",
    "import os\n",
    "import difflib \n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.util import ngrams \n",
    "from difflib import SequenceMatcher\n",
    "from string import punctuation\n",
    "from termcolor import colored\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('txt/e1a.json') as f: \n",
    "    rawData = f.read()\n",
    "\n",
    "df = pd.read_json(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1 = df.loc[0]['ocr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tests = df['ocr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text: \n",
    "    def __init__(self, raw_text, label, removeStopwords=True): \n",
    "        if type(raw_text) == list: \n",
    "            # JSTOR critical works come in lists, where each item represents a page. \n",
    "            self.text = ' \\n '.join(raw_text)\n",
    "        else: \n",
    "            self.text = raw_text\n",
    "        self.label = label\n",
    "        self.tokens = self.getTokens(removeStopwords)\n",
    "        self.trigrams = self.ngrams(3)\n",
    "        \n",
    "    def getTokens(self, removeStopwords=True): \n",
    "        \"\"\" Tokenizes the text, breaking it up into words, removing punctuation. \"\"\"\n",
    "        tokenizer = nltk.RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*') # A custom regex tokenizer. \n",
    "        #tokenizer = nltk.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+') # A custom regex tokenizer. \n",
    "        spans = list(tokenizer.span_tokenize(self.text))\n",
    "        # Take note of how many spans there are in the text\n",
    "        #print(spans)\n",
    "        self.length = spans[-1][-1] \n",
    "        tokens = tokenizer.tokenize(self.text)\n",
    "        tokens = [ token.lower() for token in tokens ] # make them lowercase\n",
    "        if not removeStopwords: \n",
    "            self.spans = spans\n",
    "            return tokens\n",
    "        tokenSpans = list(zip(tokens, spans)) # zip it up\n",
    "        stopwords = nltk.corpus.stopwords.words('english') # get stopwords\n",
    "        tokenSpans = [ token for token in tokenSpans if token[0] not in stopwords ] # remove stopwords from zip\n",
    "        self.spans = [ x[1] for x in tokenSpans ] # unzip; get spans\n",
    "        return [ x[0] for x in tokenSpans ] # unzip; get tokens\n",
    "    \n",
    "    def ngrams(self, n): \n",
    "        \"\"\" Returns ngrams for the text.\"\"\"\n",
    "        return list(ngrams(self.tokens, n))\n",
    "\n",
    "class Matcher: \n",
    "    def __init__(self, textObjA, textObjB, threshold=5, ngramSize=3, removeStopwords=True):\n",
    "        \"\"\"\n",
    "        Takes as input two Text() objects, and matches between them.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.ngramSize = ngramSize\n",
    "        \n",
    "        #self.textA, self.textB = Text(fileA, removeStopwords=removeStopwords), \\\n",
    "        #        Text(fileB, removeStopwords=removeStopwords)\n",
    "        self.textA = textObjA\n",
    "        self.textB = textObjB \n",
    "        \n",
    "        self.textAgrams = self.textA.ngrams(ngramSize)\n",
    "        self.textBgrams = self.textB.ngrams(ngramSize)\n",
    "\n",
    "        self.locationsA = []\n",
    "        self.locationsB = []\n",
    "\n",
    "    def getContext(self, text, start, length, context): \n",
    "        match = self.getTokensText(text, start, length)\n",
    "        before = self.getTokensText(text, start-context, context)\n",
    "        after = self.getTokensText(text, start+length, context)\n",
    "        match = colored(match, 'red')\n",
    "        out = \" \".join([before, match, after])\n",
    "        out = out.replace('\\n', ' ') # Replace newlines with spaces. \n",
    "        out = re.sub('\\s+', ' ', out)\n",
    "        return out\n",
    "\n",
    "    def getTokensText(self, text, start, length):  \n",
    "        \"\"\" Looks up the passage in the original text, using its spans. \"\"\"\n",
    "        matchTokens = text.tokens[start:start+length]\n",
    "        spans = text.spans[start:start+length]\n",
    "        if len(spans) == 0: \n",
    "            # Don't try to get text or context beyond the end of a text. \n",
    "            passage = \"\"\n",
    "        else: \n",
    "            passage = text.text[spans[0][0]:spans[-1][-1]]\n",
    "        return passage \n",
    "\n",
    "    def getLocations(self, text, start, length, asPercentages=False): \n",
    "        \"\"\" Gets the numeric locations of the match. \"\"\"\n",
    "        spans = text.spans[start:start+length]\n",
    "        if asPercentages: \n",
    "            locations = (spans[0][0]/text.length, spans[-1][-1]/text.length)\n",
    "        else: \n",
    "            locations = (spans[0][0], spans[-1][-1])\n",
    "        return locations\n",
    "\n",
    "    def getMatch(self, match, textA, textB, context): \n",
    "        length = match.size + self.ngramSize - 1 # offset according to nGram size \n",
    "        wordsA = self.getContext(textA, match.a, length, context)\n",
    "        wordsB = self.getContext(textB, match.b, length, context)\n",
    "        spansA = self.getLocations(textA, match.a, length)\n",
    "        spansB = self.getLocations(textB, match.b, length)\n",
    "        self.locationsA.append(spansA)\n",
    "        self.locationsB.append(spansB)\n",
    "        line1 = ('%s: %s %s' % (colored(textA.label, 'green'), spansA, wordsA) )\n",
    "        line2 = ('%s: %s %s' % (colored(textB.label, 'green'), spansB, wordsB) )\n",
    "        return line1 + '\\n' + line2\n",
    "\n",
    "    def match(self): \n",
    "        \"\"\"\n",
    "        This does the main work of finding matching n-gram sequences between\n",
    "        the texts.\n",
    "        \"\"\"\n",
    "        sequence = SequenceMatcher(None,self.textAgrams,self.textBgrams)\n",
    "        matchingBlocks = sequence.get_matching_blocks()\n",
    "\n",
    "        # Only return the matching sequences that are higher than the \n",
    "        # threshold given by the user. \n",
    "        highMatchingBlocks = [match for match in matchingBlocks if match.size > self.threshold]\n",
    "    \n",
    "        numBlocks = len(highMatchingBlocks)\n",
    "        self.numMatches = numBlocks\n",
    "        \n",
    "        if numBlocks > 0: \n",
    "            print('%s total matches found.' % numBlocks, flush=True)\n",
    "\n",
    "        for num, match in enumerate(highMatchingBlocks): \n",
    "            print('match: ', match)\n",
    "            out = self.getMatch(match, self.textA, self.textB, 5)\n",
    "            print('\\n')\n",
    "            print('match %s:' % (num+1), flush=True)\n",
    "            print(out, flush=True)\n",
    "\n",
    "        return self.numMatches, self.locationsA, self.locationsB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1Text = Text(test1, 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm = Text(open('middlemarch.txt').read(), 'Middlemarch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Match(): \n",
    "    def __init__(self, a, b, size): \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.size = size\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other): \n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "class Matcher(): \n",
    "    \"\"\" Replacement for SequenceMatcher that does fuzzy text matching.\"\"\"\n",
    "    def __init__(self, textAgrams, textBgrams): \n",
    "        self.textAgrams = textAgrams\n",
    "        self.textBgrams = textBgrams\n",
    "        self.countA = Counter(textAgrams)\n",
    "        self.countB = Counter(textBgrams)\n",
    "        # Find the intersection of the two sets. \n",
    "        self.initialMatches = list(set(textAgrams) & set(textBgrams))\n",
    "    \n",
    "    @property\n",
    "    def matchPairs(self): \n",
    "        pairs = []\n",
    "        for match in self.initialMatches: \n",
    "            # Handle multiple matches. \n",
    "            if self.countA[match] > 1: \n",
    "                aLocs = [i for i, x in enumerate(self.textAgrams) if x==match]\n",
    "                bLoc = self.textBgrams.index(match)\n",
    "                for loc in aLocs: \n",
    "                    pairs.append(Match(a=loc, b=bLoc, size=1))\n",
    "            if self.countB[match] > 1: \n",
    "                bLocs = [i for i, x in enumerate(self.textBgrams) if x==match]\n",
    "                aLoc = self.textAgrams.index(match)\n",
    "                for loc in bLocs: \n",
    "                    pairs.append(Match(a=aLoc, b=loc, size=1))\n",
    "            else: \n",
    "                pairs.append(Match(a=self.textAgrams.index(match),\n",
    "                                   b=self.textBgrams.index(match), size=1))\n",
    "        return pairs\n",
    "    \n",
    "    def mergedMatches(self): \n",
    "        \"\"\" Merge contiguous matches. \"\"\"\n",
    "        sortedMatches = sorted(self.matchPairs, key=lambda x: x.a)\n",
    "        deduped = [m for i, m in enumerate(sortedMatches[:-1])\n",
    "                  if m != sortedMatches[i+1]]\n",
    "        # Subtract indices as a way of grouping contiguous items.\n",
    "        return [(m, m.a-i, m.b-i) for i, m in enumerate(deduped)] \n",
    "                    \n",
    "            \n",
    "    @property \n",
    "    def extendedMatches(self): \n",
    "        out = []\n",
    "        for match in self.matchPairs: \n",
    "            for forward in [True, False]: \n",
    "                go = True\n",
    "                while go: \n",
    "#                     print('match: ', match)\n",
    "                    # Extend it as far as possible.\n",
    "                    extended = self.extendMatch(match, forward=forward)\n",
    "#                     print('extended: ', extended)\n",
    "                    if match == extended: \n",
    "                        go = False\n",
    "                    match = extended\n",
    "                out.append(match)\n",
    "        return out\n",
    "        \n",
    "    def extendMatch(self, match, forward):  \n",
    "        if forward: \n",
    "            offset, wordIndex, correction = match.size, -1, 0\n",
    "        else: \n",
    "            offset, wordIndex, correction = -1, 0, 1\n",
    "        try: \n",
    "            a = self.textAgrams[match.a+offset]\n",
    "            b = self.textBgrams[match.b+offset]\n",
    "        except IndexError:\n",
    "            # The extension is out of range for the document. \n",
    "            return match\n",
    "        wordA, wordB = a[wordIndex], b[wordIndex]\n",
    "#         print('a: ', a)\n",
    "#         print('b: ', b)\n",
    "        if a==b or editDistance(wordA, wordB)<2 or self.editRatio(wordA,wordB)>.9: \n",
    "            return Match(a=match.a-correction, b=match.b-correction, size=match.size+1)\n",
    "        else: \n",
    "            return match\n",
    "    \n",
    "    def editRatio(self, a, b): \n",
    "        lensum = len(a) + len(b)\n",
    "        distance = editDistance(a, b)\n",
    "        return (lensum-distance)/lensum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fuzzySeqMatcher(SequenceMatcher): \n",
    "    def __init__(self, a, b): \n",
    "        SequenceMatcher.__init__(self, a=a, b=b)\n",
    "        self.matches = self.get_matching_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = Matcher(test1Text.trigrams, mm.trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotated = m.mergedMatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, <itertools._grouper object at 0x7f42937cdcc0>)\n",
      "(924, <itertools._grouper object at 0x7f42937cdfd0>)\n",
      "(935, <itertools._grouper object at 0x7f42937cd320>)\n",
      "(981, <itertools._grouper object at 0x7f42937ce1d0>)\n",
      "(1011, <itertools._grouper object at 0x7f42937ce2b0>)\n",
      "(1013, <itertools._grouper object at 0x7f42937ce320>)\n",
      "(1012, <itertools._grouper object at 0x7f42937ce400>)\n",
      "(1037, <itertools._grouper object at 0x7f42937ce470>)\n",
      "(1045, <itertools._grouper object at 0x7f42937ce550>)\n",
      "(1079, <itertools._grouper object at 0x7f42937ce630>)\n",
      "(1119, <itertools._grouper object at 0x7f42937ce6d8>)\n",
      "(1132, <itertools._grouper object at 0x7f42937ce748>)\n",
      "(1157, <itertools._grouper object at 0x7f42937ce7b8>)\n",
      "(1207, <itertools._grouper object at 0x7f42937ce828>)\n",
      "(1211, <itertools._grouper object at 0x7f42937ce908>)\n",
      "(1217, <itertools._grouper object at 0x7f42937ce978>)\n",
      "(1231, <itertools._grouper object at 0x7f42937ce9e8>)\n",
      "(1241, <itertools._grouper object at 0x7f42937cea90>)\n",
      "(1240, <itertools._grouper object at 0x7f42937ceb38>)\n",
      "(1239, <itertools._grouper object at 0x7f42937cebe0>)\n",
      "(1249, <itertools._grouper object at 0x7f42937cec88>)\n",
      "(1262, <itertools._grouper object at 0x7f42937ced30>)\n",
      "(1373, <itertools._grouper object at 0x7f42937ceef0>)\n",
      "(1452, <itertools._grouper object at 0x7f42937ce128>)\n",
      "(1451, <itertools._grouper object at 0x7f42937cf080>)\n",
      "(1459, <itertools._grouper object at 0x7f42937cf208>)\n",
      "(1534, <itertools._grouper object at 0x7f42937cf2b0>)\n",
      "(1539, <itertools._grouper object at 0x7f42937cf400>)\n",
      "(1542, <itertools._grouper object at 0x7f42937cf4e0>)\n",
      "(1546, <itertools._grouper object at 0x7f42937cf550>)\n",
      "(1548, <itertools._grouper object at 0x7f42937cf630>)\n",
      "(1633, <itertools._grouper object at 0x7f42937cf748>)\n",
      "(1640, <itertools._grouper object at 0x7f42937cf860>)\n",
      "(1697, <itertools._grouper object at 0x7f42937cf8d0>)\n",
      "(1758, <itertools._grouper object at 0x7f42937cf9e8>)\n",
      "(1762, <itertools._grouper object at 0x7f42937cfb00>)\n",
      "(1860, <itertools._grouper object at 0x7f42937cfc18>)\n",
      "(1862, <itertools._grouper object at 0x7f42937cfcc0>)\n",
      "(1864, <itertools._grouper object at 0x7f42937cfda0>)\n",
      "(1869, <itertools._grouper object at 0x7f42937cfe80>)\n",
      "(1884, <itertools._grouper object at 0x7f42937cff28>)\n",
      "(1893, <itertools._grouper object at 0x7f42937cffd0>)\n",
      "(1910, <itertools._grouper object at 0x7f42937cf390>)\n",
      "(1919, <itertools._grouper object at 0x7f42937d20f0>)\n",
      "(2081, <itertools._grouper object at 0x7f42937d2358>)\n",
      "(2100, <itertools._grouper object at 0x7f42937d23c8>)\n",
      "(2122, <itertools._grouper object at 0x7f42937d24a8>)\n",
      "(2130, <itertools._grouper object at 0x7f42937d2630>)\n",
      "(2134, <itertools._grouper object at 0x7f42937d26a0>)\n",
      "(2152, <itertools._grouper object at 0x7f4293802828>)\n",
      "(2157, <itertools._grouper object at 0x7f42937d2668>)\n",
      "(2159, <itertools._grouper object at 0x7f42937d26d8>)\n",
      "(2161, <itertools._grouper object at 0x7f42937d2780>)\n",
      "(2163, <itertools._grouper object at 0x7f42937d2898>)\n",
      "(2167, <itertools._grouper object at 0x7f42937d29b0>)\n",
      "(2175, <itertools._grouper object at 0x7f42937d2a58>)\n",
      "(2180, <itertools._grouper object at 0x7f42937d2ac8>)\n",
      "(2184, <itertools._grouper object at 0x7f42937d2cc0>)\n",
      "(2186, <itertools._grouper object at 0x7f42937d2dd8>)\n",
      "(2188, <itertools._grouper object at 0x7f42937d2eb8>)\n",
      "(2195, <itertools._grouper object at 0x7f42937d2f98>)\n",
      "(2199, <itertools._grouper object at 0x7f42937d22b0>)\n",
      "(2202, <itertools._grouper object at 0x7f42937d3198>)\n",
      "(2236, <itertools._grouper object at 0x7f42937d3208>)\n",
      "(2240, <itertools._grouper object at 0x7f42937d32e8>)\n",
      "(2244, <itertools._grouper object at 0x7f42937d3390>)\n",
      "(2252, <itertools._grouper object at 0x7f42937d3518>)\n",
      "(2378, <itertools._grouper object at 0x7f42937d35c0>)\n",
      "(2391, <itertools._grouper object at 0x7f42937d3668>)\n",
      "(2604, <itertools._grouper object at 0x7f42937d36d8>)\n",
      "(2607, <itertools._grouper object at 0x7f42937d37f0>)\n",
      "(2610, <itertools._grouper object at 0x7f42937d3908>)\n",
      "(2624, <itertools._grouper object at 0x7f42937d3a58>)\n",
      "(2679, <itertools._grouper object at 0x7f42937d3ba8>)\n",
      "(2683, <itertools._grouper object at 0x7f42937d3c50>)\n",
      "(2697, <itertools._grouper object at 0x7f42937d3cf8>)\n",
      "(2700, <itertools._grouper object at 0x7f42937d3d68>)\n",
      "(2704, <itertools._grouper object at 0x7f42937d3eb8>)\n",
      "(2812, <itertools._grouper object at 0x7f42937cd080>)\n",
      "(2826, <itertools._grouper object at 0x7f42937d3f98>)\n",
      "(2904, <itertools._grouper object at 0x7f42937d32b0>)\n",
      "(2909, <itertools._grouper object at 0x7f42937d4240>)\n",
      "(2939, <itertools._grouper object at 0x7f42937d4390>)\n",
      "(3149, <itertools._grouper object at 0x7f42937d44e0>)\n",
      "(3148, <itertools._grouper object at 0x7f42937d45c0>)\n",
      "(3147, <itertools._grouper object at 0x7f42937d46d8>)\n",
      "(3146, <itertools._grouper object at 0x7f42937d4898>)\n",
      "(3156, <itertools._grouper object at 0x7f42937d4978>)\n",
      "(3161, <itertools._grouper object at 0x7f42937d49e8>)\n",
      "(3160, <itertools._grouper object at 0x7f42937d4ac8>)\n",
      "(3159, <itertools._grouper object at 0x7f42937d4b70>)\n",
      "(3158, <itertools._grouper object at 0x7f42937d4be0>)\n",
      "(3157, <itertools._grouper object at 0x7f42937d4c88>)\n",
      "(3156, <itertools._grouper object at 0x7f42937d4d68>)\n",
      "(3158, <itertools._grouper object at 0x7f42937d4e10>)\n",
      "(3160, <itertools._grouper object at 0x7f42937d4f60>)\n",
      "(3178, <itertools._grouper object at 0x7f42937d42e8>)\n",
      "(3190, <itertools._grouper object at 0x7f42937d50b8>)\n",
      "(3231, <itertools._grouper object at 0x7f42937d5198>)\n",
      "(3262, <itertools._grouper object at 0x7f42937d5240>)\n",
      "(3264, <itertools._grouper object at 0x7f42937d5320>)\n",
      "(3279, <itertools._grouper object at 0x7f42937d5400>)\n",
      "(3328, <itertools._grouper object at 0x7f42937d5550>)\n",
      "(3332, <itertools._grouper object at 0x7f42937d5668>)\n",
      "(3336, <itertools._grouper object at 0x7f42937d5748>)\n",
      "(3335, <itertools._grouper object at 0x7f42937d57f0>)\n",
      "(3343, <itertools._grouper object at 0x7f42937d5940>)\n",
      "(3389, <itertools._grouper object at 0x7f42937d5a20>)\n",
      "(3391, <itertools._grouper object at 0x7f42937d5ac8>)\n",
      "(3395, <itertools._grouper object at 0x7f42937d5ba8>)\n",
      "(3394, <itertools._grouper object at 0x7f42937d5c18>)\n",
      "(3426, <itertools._grouper object at 0x7f42937d5cc0>)\n",
      "(3474, <itertools._grouper object at 0x7f42937d5dd8>)\n",
      "(3476, <itertools._grouper object at 0x7f42937d5eb8>)\n",
      "(3480, <itertools._grouper object at 0x7f42937d5160>)\n",
      "(3548, <itertools._grouper object at 0x7f42937d6048>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
