{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "from nltk.corpus import names\n",
    "from nltk.metrics.distance import edit_distance as editDistance\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re \n",
    "import os\n",
    "import difflib \n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.util import ngrams \n",
    "import difflib\n",
    "from string import punctuation\n",
    "from termcolor import colored\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('txt/e1a.json') as f: \n",
    "    rawData = f.read()\n",
    "\n",
    "df = pd.read_json(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1 = df.loc[0]['ocr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tests = df['ocr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text: \n",
    "    def __init__(self, raw_text, label, removeStopwords=True): \n",
    "        if type(raw_text) == list: \n",
    "            # JSTOR critical works come in lists, where each item represents a page. \n",
    "            self.text = ' \\n '.join(raw_text)\n",
    "        else: \n",
    "            self.text = raw_text\n",
    "        self.label = label\n",
    "        self.preprocess(self.text)\n",
    "        self.tokens = self.getTokens(removeStopwords)\n",
    "        self.trigrams = self.ngrams(3)\n",
    "        \n",
    "    def preprocess(self, text): \n",
    "        \"\"\" Heals hyphenated words, and maybe other things. \"\"\"    \n",
    "        self.text = re.sub(r'([A-Za-z])- ([a-z])', r'\\1\\2', self.text)\n",
    "\n",
    "    def getTokens(self, removeStopwords=True): \n",
    "        \"\"\" Tokenizes the text, breaking it up into words, removing punctuation. \"\"\"\n",
    "        tokenizer = nltk.RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*') # A custom regex tokenizer. \n",
    "        #tokenizer = nltk.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+') # A custom regex tokenizer. \n",
    "        spans = list(tokenizer.span_tokenize(self.text))\n",
    "        # Take note of how many spans there are in the text\n",
    "        #print(spans)\n",
    "        self.length = spans[-1][-1] \n",
    "        tokens = tokenizer.tokenize(self.text)\n",
    "        tokens = [ token.lower() for token in tokens ] # make them lowercase\n",
    "        stemmer = LancasterStemmer()\n",
    "        tokens = [ stemmer.stem(token) for token in tokens ]\n",
    "        if not removeStopwords: \n",
    "            self.spans = spans\n",
    "            return tokens\n",
    "        tokenSpans = list(zip(tokens, spans)) # zip it up\n",
    "        stopwords = nltk.corpus.stopwords.words('english') # get stopwords\n",
    "        tokenSpans = [ token for token in tokenSpans if token[0] not in stopwords ] # remove stopwords from zip\n",
    "        self.spans = [ x[1] for x in tokenSpans ] # unzip; get spans\n",
    "        return [ x[0] for x in tokenSpans ] # unzip; get tokens\n",
    "    \n",
    "    def ngrams(self, n): \n",
    "        \"\"\" Returns ngrams for the text.\"\"\"\n",
    "        return list(ngrams(self.tokens, n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtendedMatch(): \n",
    "    \"\"\" \n",
    "    Data structure container for a fancy version of a difflib-style\n",
    "    Match object. The difflib Match class won't work for extended \n",
    "    matches, since it only has the properties `a` (start location in \n",
    "    text A), `b` (start location in text B), and size. Since our fancy \n",
    "    new matches have different sizes in our different texts, we'll need\n",
    "    two size attributes. \n",
    "    \"\"\"\n",
    "    def __init__(self, a, b, sizeA, sizeB): \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.sizeA = sizeA\n",
    "        self.sizeB = sizeB\n",
    "        # Whether this is actually two matches that have been fused into one. \n",
    "        self.healed = False \n",
    "        # Whether this match has been extended from its original boundaries.\n",
    "        self.extendedBackwards = 0\n",
    "        self.extendedForwards = 0\n",
    "    \n",
    "    def __repr__(self): \n",
    "        out = \"a: %s, b: %s, size a: %s, size b: %s\" % \\\n",
    "                (self.a, self.b, self.sizeA, self.sizeB)\n",
    "        if self.extendedBackwards: \n",
    "            out += \", extended backwards x%s\" % self.extendedBackwards\n",
    "        if self.extendedForwards: \n",
    "            out += \", extended forwards x%s\" % self.extendedForwards\n",
    "        if self.healed: \n",
    "            out += \", healed\"\n",
    "        return out\n",
    "\n",
    "class Matcher(): \n",
    "    def __init__(self, textObjA, textObjB, threshold=5, ngramSize=3, removeStopwords=True):\n",
    "         \n",
    "        \"\"\"\n",
    "        Takes as input two Text() objects, and matches between them.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.ngramSize = ngramSize\n",
    "        \n",
    "        #self.textA, self.textB = Text(fileA, removeStopwords=removeStopwords), \\\n",
    "        #        Text(fileB, removeStopwords=removeStopwords)\n",
    "        self.textA = textObjA\n",
    "        self.textB = textObjB \n",
    "        \n",
    "        self.textAgrams = self.textA.ngrams(ngramSize)\n",
    "        self.textBgrams = self.textB.ngrams(ngramSize)\n",
    "\n",
    "        self.locationsA = []\n",
    "        self.locationsB = []\n",
    "        \n",
    "        self.initial_matches = self.get_initial_matches()\n",
    "        self.healed_matches = self.heal_neighboring_matches()\n",
    "        \n",
    "        self.extended_matches = self.extend_matches()\n",
    "    \n",
    "    def get_initial_matches(self): \n",
    "        \"\"\"\n",
    "        This does the main work of finding matching n-gram sequences between\n",
    "        the texts.\n",
    "        \"\"\"\n",
    "        sequence = SequenceMatcher(None,self.textAgrams,self.textBgrams)\n",
    "        matchingBlocks = sequence.get_matching_blocks()\n",
    "        \n",
    "        # Only return the matching sequences that are higher than the \n",
    "        # threshold given by the user. \n",
    "        highMatchingBlocks = [match for match in matchingBlocks if match.size > self.threshold]\n",
    "    \n",
    "        numBlocks = len(highMatchingBlocks)\n",
    "        self.numMatches = numBlocks\n",
    "        \n",
    "        if numBlocks > 0: \n",
    "            print('%s total matches found.' % numBlocks, flush=True)\n",
    "        \n",
    "        return highMatchingBlocks\n",
    "    \n",
    "    def getContext(self, text, start, length, context): \n",
    "        match = self.getTokensText(text, start, length)\n",
    "        before = self.getTokensText(text, start-context, context)\n",
    "        after = self.getTokensText(text, start+length, context)\n",
    "        match = colored(match, 'red')\n",
    "        out = \" \".join([before, match, after])\n",
    "        out = out.replace('\\n', ' ') # Replace newlines with spaces. \n",
    "        out = re.sub('\\s+', ' ', out)\n",
    "        return out\n",
    "\n",
    "    def getTokensText(self, text, start, length):  \n",
    "        \"\"\" Looks up the passage in the original text, using its spans. \"\"\"\n",
    "        matchTokens = text.tokens[start:start+length]\n",
    "        spans = text.spans[start:start+length]\n",
    "        if len(spans) == 0: \n",
    "            # Don't try to get text or context beyond the end of a text. \n",
    "            passage = \"\"\n",
    "        else: \n",
    "            passage = text.text[spans[0][0]:spans[-1][-1]]\n",
    "        return passage \n",
    "\n",
    "    def getLocations(self, text, start, length, asPercentages=False): \n",
    "        \"\"\" Gets the numeric locations of the match. \"\"\"\n",
    "        spans = text.spans[start:start+length]\n",
    "        if asPercentages: \n",
    "            locations = (spans[0][0]/text.length, spans[-1][-1]/text.length)\n",
    "        else: \n",
    "            locations = (spans[0][0], spans[-1][-1])\n",
    "        return locations\n",
    "\n",
    "    def getMatch(self, match, context=5):\n",
    "        textA, textB = self.textA, self.textB\n",
    "        lengthA = match.sizeA + self.ngramSize -1 # offset according to nGram size\n",
    "        lengthB = match.sizeB + self.ngramSize -1 # offset according to nGram size\n",
    "        wordsA = self.getContext(textA, match.a, lengthA, context)\n",
    "        wordsB = self.getContext(textB, match.b, lengthB, context)\n",
    "        spansA = self.getLocations(textA, match.a, lengthA)\n",
    "        spansB = self.getLocations(textB, match.b, lengthB)\n",
    "        self.locationsA.append(spansA)\n",
    "        self.locationsB.append(spansB)\n",
    "        line1 = ('%s: %s %s' % (colored(textA.label, 'green'), spansA, wordsA) )\n",
    "        line2 = ('%s: %s %s' % (colored(textB.label, 'green'), spansB, wordsB) )\n",
    "        out = line1 + '\\n' + line2\n",
    "        return out\n",
    "\n",
    "    def heal_neighboring_matches(self, minDistance=8): \n",
    "        healedMatches = []\n",
    "        ignoreNext = False\n",
    "        matches = self.initial_matches.copy()\n",
    "        for i, match in enumerate(self.initial_matches): \n",
    "            if i+1 > len(self.initial_matches)-1: \n",
    "                break\n",
    "            nextMatch = self.initial_matches[i+1]\n",
    "            if ignoreNext: \n",
    "                ignoreNext = False\n",
    "                continue\n",
    "            else: \n",
    "                if ( nextMatch.a - (match.a + match.size) ) < minDistance: \n",
    "                    print('Potential healing candidate found: ')\n",
    "                    print(match, nextMatch)\n",
    "                    sizeA = (nextMatch.a + nextMatch.size) - match.a\n",
    "                    sizeB = (nextMatch.b + nextMatch.size) - match.b\n",
    "                    healed = ExtendedMatch(match.a, match.b, sizeA, sizeB)\n",
    "                    healed.healed = True\n",
    "                    healedMatches.append(healed)\n",
    "                    ignoreNext = True\n",
    "                else: \n",
    "                    sizeA, sizeB = match.size, match.size\n",
    "                    match = ExtendedMatch(match.a, match.b, sizeA, sizeB)\n",
    "                    healedMatches.append(match)\n",
    "        return healedMatches\n",
    "            \n",
    "    def edit_ratio(self, wordA, wordB): \n",
    "        \"\"\" Computes the number of edits required to transform one\n",
    "        (stemmed already, probably) word into another word, and\n",
    "        adjusts for the average number of letters in each. \n",
    "        \n",
    "        Examples: \n",
    "        color, colour: 0.1818181818\n",
    "        theater, theatre: 0.2857\n",
    "        day, today: 0.5\n",
    "        foobar, foo56bar: 0.2857\n",
    "        \"\"\" \n",
    "        distance = editDistance(wordA, wordB)\n",
    "        averageLength = (len(wordA) + len(wordB))/2\n",
    "        return distance/averageLength\n",
    "    \n",
    "    def extend_matches(self, cutoff=0.4): \n",
    "        extended = False\n",
    "        for match in self.healed_matches: \n",
    "            # Look one word before. \n",
    "            wordA = self.textAgrams[(match.a - 1)][0]\n",
    "            wordB = self.textBgrams[(match.b - 1)][0]\n",
    "            if self.edit_ratio(wordA, wordB) < cutoff: \n",
    "                print('Extending match backwards with words: %s %s' % \n",
    "                     (wordA, wordB) )\n",
    "                match.a -= 1\n",
    "                match.b -= 1\n",
    "                match.sizeA += 1\n",
    "                match.sizeB += 1\n",
    "                match.extendedBackwards += 1\n",
    "                extended = True\n",
    "            # Look one word after. \n",
    "            wordA = self.textAgrams[(match.a + match.sizeA + 1)][-1]\n",
    "            wordB = self.textBgrams[(match.b + match.sizeB + 1)][-1]\n",
    "            if self.edit_ratio(wordA, wordB) < cutoff: \n",
    "                print('Extending match forwards with words: %s %s' % \n",
    "                     (wordA, wordB) )\n",
    "                match.sizeA += 1\n",
    "                match.sizeB += 1\n",
    "                match.extendedForwards += 1\n",
    "                extended = True\n",
    "        \n",
    "        if extended: \n",
    "            # If we've gone through the whole list and there's nothing\n",
    "            # left to extend, then stop. Otherwise do this again. \n",
    "            self.extend_matches()\n",
    "            \n",
    "        return self.healed_matches\n",
    "    \n",
    "\n",
    "    def match(self): \n",
    "        \"\"\" Gets and prints all matches. \"\"\"\n",
    "        \n",
    "        for num, match in enumerate(self.extended_matches): \n",
    "            print('match: ', match)\n",
    "            out = self.getMatch(match)\n",
    "            print('\\n')\n",
    "            print('match %s:' % (num+1), flush=True)\n",
    "            print(out, flush=True)\n",
    "\n",
    "        return self.numMatches, self.locationsA, self.locationsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1Text = Text(test1, 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm = Text(open('middlemarch.txt').read(), 'Middlemarch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 total matches found.\n",
      "Potential healing candidate found: \n",
      "Match(a=820, b=1007, size=9) Match(a=834, b=1063, size=13)\n",
      "Potential healing candidate found: \n",
      "Match(a=157584, b=4386, size=12) Match(a=157599, b=4401, size=15)\n",
      "Extending match forwards with words: destiny destiny\n",
      "Extending match forwards with words: stand stand\n",
      "Extending match forwards with words: sarcast sarcast\n",
      "Extending match forwards with words: dram dram\n",
      "Extending match forwards with words: persona persona\n",
      "Extending match forwards with words: fold fold\n",
      "Extending match forwards with words: hand hand\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(mm, test1Text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
